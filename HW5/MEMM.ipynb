{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require \"hdf5\"\n",
    "require \"optim\"\n",
    "require \"nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = hdf5.open(\"data.hdf5\", \"r\")\n",
    "\n",
    "X_train = f:read(\"X_train\"):all()\n",
    "Y_train = f:read(\"Y_train\"):all()\n",
    "X_valid = f:read(\"X_valid\"):all()\n",
    "Y_valid = f:read(\"Y_valid\"):all()\n",
    "X_test = f:read(\"X_test\"):all()\n",
    "nwords = f:read(\"nwords\"):all()[1]\n",
    "nclasses = f:read(\"nclasses\"):all()[1]\n",
    "\n",
    "\n",
    "--sentences\n",
    "X_valid_sen = f:read(\"X_valid_sen\"):all()\n",
    "X_test_sen = f:read(\"X_test_sen\"):all()\n",
    "\n",
    "--MEMM\n",
    "X_train_MEMM = f:read(\"X_train_MEMM\"):all()\n",
    "X_valid_MEMM = f:read(\"X_valid_MEMM\"):all()\n",
    "X_valid_sen_MEMM = f:read(\"X_valid_sen_MEMM\"):all()\n",
    "nfeaturesMEMM = f:read(\"nfeaturesMEMM\"):all()[1]\n",
    "\n",
    "C = nclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function process(inputs, numwords)\n",
    "    processed = inputs:clone()\n",
    "    for i = 1, inputs:size(1) do\n",
    "        processed[i][2] = inputs[i][2] + numwords\n",
    "    end\n",
    "    return processed\n",
    "end\n",
    "\n",
    "function feval(w_new)\n",
    "   \n",
    "    bsize= batch\n",
    "    local idx = torch.randperm(X_:size(1)):sub(1,bsize)\n",
    "\n",
    "    local x = torch.Tensor(bsize, X_:size(2))\n",
    "\n",
    "    \n",
    "    --y = y:reshape(Y_train:size(1), 1)\n",
    "    \n",
    "    local y_ = torch.Tensor(bsize, 1)\n",
    "\n",
    "    \n",
    "    for i=1,bsize do\n",
    "        x[i] = X_[idx[i]]\n",
    "        \n",
    "        y_[i] = y[idx[i]]\n",
    "    end\n",
    "    \n",
    "    y_ = y_:squeeze()\n",
    "    local inputs = x\n",
    "    local targets = y_\n",
    "    -- reset gradients (gradients are always accumulated, to accommodate\n",
    "    -- batch methods)\n",
    "    dl_dw:zero()\n",
    "    -- evaluate the loss function and its derivative with respect to x, given a mini batch\n",
    "    local prediction = h:forward(inputs)\n",
    "    local loss_w = mse:forward(prediction, targets)\n",
    "    h:backward(inputs, mse:backward(prediction, targets))\n",
    "    \n",
    "    return loss_w, dl_dw\n",
    "            \n",
    "end\n",
    "\n",
    "function predict(input, index, network)\n",
    "    logscore = torch.log(nn.SoftMax():forward(h:forward(input:sub(index,index))))\n",
    "    return logscore:expand(C,C)\n",
    "end\n",
    "\n",
    "function viterbi(observations, logscore)\n",
    "    local initial = torch.zeros(nclasses,1) + .00001\n",
    "    initial[8] = 1.0\n",
    "    initial = initial / torch.sum(initial)\n",
    "    \n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = torch.log(initial):max(2)--(init+ emi[observations[1]]):max(2)\n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "        pred = predict(observations, i, logscore)\n",
    "        scores = pred + maxes:view(1, C):expand(C, C)\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "        end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "    \n",
    "    return classes:sub(1,-1)\n",
    "end\n",
    "\n",
    "function predict_tags(X)\n",
    "    local predictions = torch.zeros(X:size(1), X:size(2))\n",
    "    for i=1,X:size(1) do\n",
    "        local sen = (X[i]:sub(1,torch.nonzero(X[{{i,i},{},{1,1}}]):size(1))):squeeze()\n",
    "   \n",
    "        local p = viterbi(sen, predict)\n",
    "        \n",
    "        predictions[{{i,i},{1,p:size(1)}}] = p\n",
    "    end\n",
    "    return predictions\n",
    "end\n",
    "\n",
    "function predict_fscore(p_tags, y_tags)\n",
    "    local rel_retrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    local rel_notretrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    local irrel_retrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    for i=1,p_tags:size(1) do\n",
    "        for j=1,p_tags:size(2) do\n",
    "            for tag=1,5 do        \n",
    "                if y_tags[i][j] == tag then\n",
    "                    rel_notretrived[tag] = rel_notretrived[tag] + 1\n",
    "                    if p_tags[i][j] == tag then\n",
    "                        rel_retrived[tag] = rel_retrived[tag] + 1\n",
    "                    else\n",
    "                        irrel_retrived[tag] = irrel_retrived[tag] + 1\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    local recall = 0\n",
    "    local precis = 0\n",
    "    for i=1,5 do\n",
    "        local r = (rel_retrived[i]/(rel_retrived[i]+rel_notretrived[i]))\n",
    "        local p = (rel_retrived[i]/(rel_retrived[i]+irrel_retrived[i]))\n",
    "        print(i, p, r, 2*p*r/(p+r))\n",
    "        recall = recall + r\n",
    "        precis = precis + p\n",
    "    end\n",
    "    return recall/5,precis/5\n",
    "end\n",
    "\n",
    "-- Returns validation tags in format similar to p_tags\n",
    "-- valid : correct tags in sequence\n",
    "function format_valid(valid)\n",
    "    local y_tags = torch.zeros(X_valid_sen:size(1), X_valid_sen:size(2))\n",
    "    local sen_idx = 0\n",
    "    local col_idx = 0\n",
    "    for i=1,valid:size(1) do\n",
    "        if valid[i] == 8 then\n",
    "            sen_idx = sen_idx + 1\n",
    "            col_idx = 1\n",
    "        end\n",
    "        y_tags[sen_idx][col_idx] = valid[i]\n",
    "        col_idx = col_idx + 1\n",
    "    end\n",
    "    return y_tags\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "bad argument #2 to '?' (out of range at /tmp/luarocks_torch-scm-1-8597/torch7/generic/Tensor.c:888)\nstack traceback:\n\t[C]: at 0x0f82b1c0\n\t[C]: in function '__index'\n\t[string \"function process(inputs, numwords)...\"]:103: in function 'predict_fscore'\n\t[string \"y_tags = format_valid(Y_valid)...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f61ebd0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "bad argument #2 to '?' (out of range at /tmp/luarocks_torch-scm-1-8597/torch7/generic/Tensor.c:888)\nstack traceback:\n\t[C]: at 0x0f82b1c0\n\t[C]: in function '__index'\n\t[string \"function process(inputs, numwords)...\"]:103: in function 'predict_fscore'\n\t[string \"y_tags = format_valid(Y_valid)...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f61ebd0"
     ]
    }
   ],
   "source": [
    "y_tags = format_valid(Y_valid)\n",
    "r, p = predict_fscore(predictions, y_tags)\n",
    "print(r, p)\n",
    "print(2*r*p/(r+p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 12\n",
       " 63\n",
       "  2\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =X_valid_sen_MEMM\n",
    "print((a:sub(1,12)):size())\n",
    "prediction = viterbi(a[1]:sub(1,12), h)\n",
    "predictions = predict_tags(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i = 1,X_valid_sen_MEMM:size(1) do\n",
    "    X_valid_sen_MEMM[i] = process(X_valid_sen_MEMM[i], nwords)\n",
    "end\n",
    "    \n",
    "X_ = process(X_train_MEMM, nwords)\n",
    "y = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addB = nn.Add(nclasses)\n",
    "lookup = nn.LookupTable(nwords+nclasses,nclasses)\n",
    "sum = nn.Sum(2)\n",
    "softmax = nn.LogSoftMax()\n",
    "h = nn.Sequential()\n",
    "h:add(lookup)\n",
    "h:add(sum)\n",
    "h:add(addB)\n",
    "mse = nn.CrossEntropyCriterion()\n",
    "\n",
    "w, dl_dw = h:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss for iteration 1000 is 0.13807728519501\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 2000 is 0.18391838457066\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 3000 is 0.13537051085061\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 4000 is 0.17173982300805\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 5000 is 0.13438043089114\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 6000 is 0.16716814625599\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 7000 is 0.16481424752107\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 8000 is 0.14645104266739\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 9000 is 0.15286117048661\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 10000 is 0.12177535723676\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 11000 is 0.15619939689202\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 12000 is 0.13824664835059\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 13000 is 0.13835488520395\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 14000 is 0.14920799142582\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 15000 is 0.19176838647054\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 16000 is 0.14184333306882\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 17000 is 0.11662012019266\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 18000 is 0.13911576906433\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 19000 is 0.13204466158571\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "loss for iteration 20000 is 0.080086537032302\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 1000\n",
    "\n",
    "-- cycle on data\n",
    "for i = 1,20000 do\n",
    "    -- train a mini_batch of batchSize in parallel\n",
    "    _, fs = optim.adam(feval,w)\n",
    "\n",
    "    if i % 1000 == 0 then\n",
    "        print('loss for iteration ' .. i  .. ' is ' .. fs[1] )\n",
    "        -- print(sgd_params)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "/Users/Vincent/.luarocks/share/lua/5.1/nn/LookupTable.lua:56: index out of range at /tmp/luarocks_torch-scm-1-8597/torch7/lib/TH/generic/THTensorMath.c:141\nstack traceback:\n\t[C]: in function 'index'\n\t/Users/Vincent/.luarocks/share/lua/5.1/nn/LookupTable.lua:56: in function 'updateOutput'\n\t/Users/Vincent/.luarocks/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'\n\t[string \"function process(inputs, numwords)...\"]:44: in function 'predict'\n\t[string \"function process(inputs, numwords)...\"]:64: in function 'viterbi'\n\t[string \"a =X_test_sen...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f61ebd0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "/Users/Vincent/.luarocks/share/lua/5.1/nn/LookupTable.lua:56: index out of range at /tmp/luarocks_torch-scm-1-8597/torch7/lib/TH/generic/THTensorMath.c:141\nstack traceback:\n\t[C]: in function 'index'\n\t/Users/Vincent/.luarocks/share/lua/5.1/nn/LookupTable.lua:56: in function 'updateOutput'\n\t/Users/Vincent/.luarocks/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'\n\t[string \"function process(inputs, numwords)...\"]:44: in function 'predict'\n\t[string \"function process(inputs, numwords)...\"]:64: in function 'viterbi'\n\t[string \"a =X_test_sen...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010f61ebd0"
     ]
    }
   ],
   "source": [
    "a =X_test_sen\n",
    "prediction = viterbi(a[1]:sub(1,12), h)\n",
    "predictions = predict_tags(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Writes to file in Kaggle prediction format\n",
    "-- p : prediction tensor\n",
    "-- fname : output filename\n",
    "function kaggle_format(p, fname)\n",
    "    local f = io.open(\"predictions/\" .. fname, \"w\")\n",
    "    f:write(\"ID,Labels\\n\")\n",
    "    for i=1,p:size(1) do\n",
    "        local s = \"\"\n",
    "        local t = {[1]=\"O\",[2]=\"PER\",[3]=\"LOC\",[4]=\"ORG\",[5]=\"MISC\",[6]=\"MISC\",[7]=\"LOC\",[8]=\"O\",[9]=\"O\"}\n",
    "        local prev_tag = 0\n",
    "        local prev_idx = 0\n",
    "        for j=2,p:size(2) do\n",
    "            if p[i][j] == 0 then\n",
    "                break\n",
    "            end\n",
    "            if p[i][j] ~= 1 then\n",
    "                if prev_tag == 0 then\n",
    "                    s = s .. t[p[i][j]] .. \"-\" .. tostring(j-1)\n",
    "                elseif p[i][j] == prev_tag and prev_idx == j-1 then\n",
    "                    s = s .. \"-\" .. tostring(j-1) \n",
    "                else\n",
    "                    s = s .. \" \" .. t[p[i][j]] .. \"-\" .. tostring(j-1)\n",
    "                end\n",
    "                prev_tag = p[i][j]\n",
    "                prev_idx = j\n",
    "            end\n",
    "        end\n",
    "        f:write(i .. \",\" .. s .. \"\\n\")\n",
    "    end\n",
    "    f:close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_format(predictions, \"pred_\" .. tostring(os.time()) .. \"_hmm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 26\n",
       " 1  1  2  4  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 3  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 3  5  1  1  2  1  1  1  1  1  1  1  4  1  4  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  2  1  1  1  4  1  1  1  4  1  1  1  1  1  1  4  1\n",
       " 1  1  4  1  1  1  1  1  1  1  1  1  1  1  4  4  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  4  1  1  1  1  1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  0\n",
       " 4  1  1  1  1  1  1  1  1  1  1  1  1  2  1  2  2  1  1  1  1  1  1  1  1  1\n",
       " 2  1  1  1  1  3  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  1  1\n",
       " 1  1  1  4  1  1  1  1  1  1  1  1  1  4  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  3  1  4  1  2  2  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "Columns 27 to 52\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  3  1  2  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 4  1  3  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  1  0  0  0  0  0  0  0  0  0  0\n",
       "\n",
       "Columns 53 to 63\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       "[torch.DoubleTensor of size 10x63]\n",
       "\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions:sub(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 26\n",
       " 8  1  1  4  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 8  3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 8  5  5  1  2  2  1  1  1  1  1  1  1  4  1  4  1  1  1  1  1  1  1  1  1  1\n",
       " 8  1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  4  1  4  1  1  1  1  1  1  4\n",
       " 8  1  1  4  1  1  1  1  1  1  1  1  3  3  1  4  1  1  1  1  1  1  1  1  1  1\n",
       " 8  1  1  1  1  4  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  0\n",
       " 8  4  1  1  1  1  1  1  1  1  1  1  1  2  2  1  2  2  1  1  1  1  1  1  1  1\n",
       " 8  2  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  1\n",
       " 8  1  1  1  4  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1\n",
       " 8  1  1  3  1  4  1  2  2  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "Columns 27 to 52\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  3  1  2  2  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  4  1  3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  0  0  0  0  0  0  0  0  0  0\n",
       "\n",
       "Columns 53 to 63\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0\n",
       "[torch.DoubleTensor of size 10x63]\n",
       "\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tags:sub(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
