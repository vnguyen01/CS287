{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require \"hdf5\"\n",
    "require \"optim\"\n",
    "require \"nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = hdf5.open(\"data.hdf5\", \"r\")\n",
    "\n",
    "X_train = f:read(\"X_train\"):all()\n",
    "Y_train = f:read(\"Y_train\"):all()\n",
    "X_valid = f:read(\"X_valid\"):all()\n",
    "Y_valid = f:read(\"Y_valid\"):all()\n",
    "X_test = f:read(\"X_test\"):all()\n",
    "nwords = f:read(\"nwords\"):all()[1]\n",
    "nclasses = f:read(\"nclasses\"):all()[1]\n",
    "\n",
    "--a minor hack to avoid changing variable \n",
    "C = nclasses\n",
    "nfeatures = f:read(\"nfeatures\"):all()[1]\n",
    "\n",
    "--sentences\n",
    "X_valid_sen = f:read(\"X_valid_sen\"):all()\n",
    "X_test_sen = f:read(\"X_test_sen\"):all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 8\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 26]\n",
       "\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--Fits the count-based model with given smoothing parameters\n",
    "--X : sequence features\n",
    "--Y : sequence labels\n",
    "--alpha1 : additive alpha for class counts\n",
    "--alpha2 : additive alpha for class-conditional feature counts\n",
    "function fit(X, Y, alpha1, alpha2)\n",
    "    --count matrix of class transitions: p(y_i|y_{i-1},\\theta)\n",
    "    local C_trans = torch.ones(nclasses,nclasses)*alpha1\n",
    "    --count matrix of class-conditional features: p(x_i|y_i,\\theta)\n",
    "    local C_emi = torch.ones(nclasses, nwords)*alpha2\n",
    "    \n",
    "    for i = 2,X:size(1) do \n",
    "        local y_curr = Y[i]\n",
    "        local y_prev = Y[i-1]\n",
    "        local x_curr = X[i]\n",
    "        C_trans[y_prev][y_curr] = C_trans[y_prev][y_curr] + 1\n",
    "        C_emi[y_curr][x_curr] = C_emi[y_curr][x_curr] + 1\n",
    "    end\n",
    "    C_trans:cdiv((C_trans:sum(2)):expand(C_trans:size(1),C_trans:size(2)))\n",
    "    C_emi:cdiv((C_emi:sum(2)):expand(C_emi:size(1),C_emi:size(2)))\n",
    "    return C_trans:log():t(), C_emi:log():t()\n",
    "end\n",
    "\n",
    "-- log-scores of transition and emission\n",
    "-- corresponds to the vector y in the lecture notes\n",
    "-- i: timestep for the computed score\n",
    "function score_hmm(observations, i, trans, emi)\n",
    "    local observation_emission = emi[observations[i]]:reshape(C, 1):expand(C, C)\n",
    "    -- NOTE: allocates a new Tensor\n",
    "    return observation_emission + trans\n",
    "end\n",
    "\n",
    "-- Viterbi algorithm.\n",
    "-- observations: a sequence of observations, represented as integers\n",
    "-- logscore: the edge scoring function over classes and observations in a history-based model\n",
    "function viterbi(observations, logscore, trans, emi, init)\n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = (init + emi[observations[1]]):max(2)\n",
    "   \n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "        y = logscore(observations, i, trans, emi)\n",
    "        scores = y + maxes:view(1, C):expand(C, C)\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "        end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "    return classes\n",
    "end\n",
    "\n",
    "index = 11\n",
    "initial = torch.zeros(nclasses,1)\n",
    "initial[1] = 1.0\n",
    "sen = X_valid_sen[index]:sub(1,torch.nonzero(X_valid_sen[index]):size(1))\n",
    "print(viterbi(sen, score_hmm, ct, ce, initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"-- Viterbi algorithm....\"]:11: attempt to index local 'init' (a nil value)\nstack traceback:\n\t[string \"-- Viterbi algorithm....\"]:11: in function 'viterbi'\n\t[string \"ct, ce = fit(X_train, Y_train, 1, 1)...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010ebb7bd0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- Viterbi algorithm....\"]:11: attempt to index local 'init' (a nil value)\nstack traceback:\n\t[string \"-- Viterbi algorithm....\"]:11: in function 'viterbi'\n\t[string \"ct, ce = fit(X_train, Y_train, 1, 1)...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/Vincent/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/Vincent/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/Vincent/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010ebb7bd0"
     ]
    }
   ],
   "source": [
    "ct, ce = fit(X_train, Y_train, 1, 1)\n",
    "index = 2\n",
    "sen = X_valid_sen[index]:sub(1,torch.nonzero(X_valid_sen[index]):size(1))\n",
    "print(viterbi(sen, score_hmm, ct, ce))\n",
    "--print(viterbi(X_valid_sen[20], score_hmm, ct, ce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    1\n",
       " 1964\n",
       "  630\n",
       " 8763\n",
       " 6403\n",
       " 1924\n",
       " 1617\n",
       " 8764\n",
       " 2093\n",
       " 8765\n",
       " 8766\n",
       "   11\n",
       "[torch.DoubleTensor of size 12]\n",
       "\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_sen[1]:sub(1,torch.nonzero(X_valid_sen[1]):size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    1\n",
       " 1964\n",
       "  630\n",
       " 8763\n",
       " 6403\n",
       " 1924\n",
       " 1617\n",
       " 8764\n",
       " 2093\n",
       " 8765\n",
       " 8766\n",
       "   11\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "    0\n",
       "[torch.DoubleTensor of size 63]\n",
       "\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_sen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--Returns a distribution over the various tags\n",
    "--y_prev : previous class tag\n",
    "--x_curr : current feature \n",
    "--C_trans : transition count matrix\n",
    "--C_emi : emission count matrix\n",
    "function predict_distri(y_prev, x_curr, C_trans, C_emi)\n",
    "    --compute transition and emission distributions\n",
    "    local trans = C_trans[y_prev]/torch.sum(C_trans[y_prev])\n",
    "    local emi = C_emi[{{},{x_curr,x_curr}}]/torch.sum(C_emi[{{},{x_curr,x_curr}}])\n",
    "    return torch.log(trans) + torch.log(emi)\n",
    "end\n",
    "\n",
    "--Returns a log-probability table for a given single sentence and labels\n",
    "--sen : a single sentence with word features\n",
    "--labels : corresponding target labels for a sentence\n",
    "--C_trans : transition count matrix\n",
    "--C_emi : emission count matrix\n",
    "function predict_table(sen, labels, C_trans, C_emi)\n",
    "    local table = torch.ones(X_valid_sen:size()[2],nclasses)\n",
    "    for j=1,sen:size()[1] do\n",
    "        local feat = sen[j]\n",
    "        --end of sentence\n",
    "        if feat == 0 then\n",
    "            break\n",
    "        end\n",
    "        table[j] = predict_distri(2, 15, C_trans, C_emi)\n",
    "    end\n",
    "    return table:t()\n",
    "end\n",
    "\n",
    "function viterbi(proba_table)\n",
    "    \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
