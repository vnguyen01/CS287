{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require \"hdf5\"\n",
    "require \"optim\"\n",
    "require \"nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = hdf5.open(\"data.hdf5\", \"r\")\n",
    "\n",
    "X_train = f:read(\"X_train\"):all()\n",
    "Y_train = f:read(\"Y_train\"):all()\n",
    "X_valid = f:read(\"X_valid\"):all()\n",
    "Y_valid = f:read(\"Y_valid\"):all()\n",
    "X_test = f:read(\"X_test\"):all()\n",
    "nwords = f:read(\"nwords\"):all()[1]\n",
    "nclasses = f:read(\"nclasses\"):all()[1]\n",
    "\n",
    "--a minor hack to avoid changing variable names\n",
    "C = nclasses\n",
    "nfeatures = f:read(\"nfeaturesHMM\"):all()[1]\n",
    "\n",
    "--sentences\n",
    "X_valid_sen = f:read(\"X_valid_sen\"):all()\n",
    "X_test_sen = f:read(\"X_test_sen\"):all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--Fits the count-based model with given smoothing parameters\n",
    "--X : sequence features\n",
    "--Y : sequence labels\n",
    "--alpha1 : additive alpha for class counts\n",
    "--alpha2 : additive alpha for class-conditional feature counts\n",
    "function hmm_fit(X, Y, alpha1, alpha2)\n",
    "    --count matrix of class transitions: p(y_i|y_{i-1},\\theta)\n",
    "    local C_trans = torch.ones(nclasses,nclasses)*alpha1\n",
    "    --count matrix of class-conditional features: p(x_i|y_i,\\theta)\n",
    "    local C_emi = torch.ones(nclasses, nwords)*alpha2\n",
    "    for i = 2,X:size(1) do \n",
    "        local y_curr = Y[i]\n",
    "        local y_prev = Y[i-1]\n",
    "        local x_curr = X[i][1]\n",
    "        C_trans[y_prev][y_curr] = C_trans[y_prev][y_curr] + 1\n",
    "        C_emi[y_curr][x_curr] = C_emi[y_curr][x_curr] + 1\n",
    "    end\n",
    "    C_trans:cdiv(((C_trans:sum(2)):expand(C_trans:size(1),C_trans:size(2)))*(alpha1))\n",
    "    C_emi:cdiv(((C_emi:sum(2)):expand(C_emi:size(1),C_emi:size(2)))*(alpha2))\n",
    "    return C_trans:log():t(), C_emi:log():t()\n",
    "end\n",
    "\n",
    "-- log-scores of transition and emission\n",
    "-- corresponds to the vector y in the lecture notes\n",
    "-- i: timestep for the computed score\n",
    "function score_hmm(observations, i, trans, emi)\n",
    "    local observation_emission = emi[observations[i]]:reshape(C, 1):expand(C, C)\n",
    "    -- NOTE: allocates a new Tensor\n",
    "    return observation_emission + trans\n",
    "end\n",
    "\n",
    "-- Viterbi algorithm.\n",
    "-- observations: a sequence of observations, represented as integers\n",
    "-- logscore: the edge scoring function over classes and observations in a history-based model\n",
    "function viterbi(observations, logscore, trans, emi)\n",
    "    local initial = torch.zeros(nclasses,1) + .000001\n",
    "    initial[8] = 1.0\n",
    "    initial = initial / torch.sum(initial)\n",
    "    \n",
    "    local n = observations:size(1)\n",
    "    local max_table = torch.Tensor(n, C)\n",
    "    local backpointer_table = torch.Tensor(n, C)\n",
    "    -- first timestep\n",
    "    -- the initial most likely paths are the initial state distribution\n",
    "    -- NOTE: another unnecessary Tensor allocation here\n",
    "    local maxes, backpointers = torch.log(initial):max(2)--(init+ emi[observations[1]]):max(2)\n",
    "    max_table[1] = maxes\n",
    "    -- remaining timesteps (\"forwarding\" the maxes)\n",
    "    for i=2,n do\n",
    "        -- precompute edge scores\n",
    "        y = logscore(observations, i, trans, emi)\n",
    "        scores = y + maxes:view(1, C):expand(C, C)\n",
    "        -- compute new maxes (NOTE: another unnecessary Tensor allocation here)\n",
    "        maxes, backpointers = scores:max(2)\n",
    "        -- record\n",
    "        max_table[i] = maxes\n",
    "        backpointer_table[i] = backpointers\n",
    "        end\n",
    "    -- follow backpointers to recover max path\n",
    "    local classes = torch.Tensor(n)\n",
    "    maxes, classes[n] = maxes:max(1)\n",
    "    for i=n,2,-1 do\n",
    "        classes[i-1] = backpointer_table[{i, classes[i]}]\n",
    "    end\n",
    "    return classes:sub(1,-1)\n",
    "end\n",
    "\n",
    "-- Returns a table of predicted tags for each sequence\n",
    "-- X : tensor of sequences by word features\n",
    "function predict_tags(X, ct, ce)\n",
    "    local predictions = torch.zeros(X:size(1), X:size(2))\n",
    "    for i=1,X:size(1) do\n",
    "        local sen = (X[i]:sub(1,torch.nonzero(X[i]):size(1))):squeeze()\n",
    "        \n",
    "        local p = viterbi(sen, score_hmm, ct, ce)\n",
    "        predictions[{{i,i},{1,p:size(1)}}] = p\n",
    "    end\n",
    "    return predictions\n",
    "end\n",
    "\n",
    "-- Returns validation tags in format similar to p_tags\n",
    "-- valid : correct tags in sequence\n",
    "function format_valid(valid)\n",
    "    local y_tags = torch.zeros(X_valid_sen:size(1), X_valid_sen:size(2))\n",
    "    local sen_idx = 0\n",
    "    local col_idx = 0\n",
    "    for i=1,valid:size(1) do\n",
    "        if valid[i] == 8 then\n",
    "            sen_idx = sen_idx + 1\n",
    "            col_idx = 1\n",
    "        end\n",
    "        y_tags[sen_idx][col_idx] = valid[i]\n",
    "        col_idx = col_idx + 1\n",
    "    end\n",
    "    return y_tags\n",
    "end\n",
    "\n",
    "-- Returns counts of retrived relevance, unretrived relevance, and retrived irrelevance for supervised data\n",
    "-- p_tags : tensor of predicted tags for each sequence\n",
    "-- y_tags : labeled tags for each sequence\n",
    "function predict_fscore(p_tags, y_tags)\n",
    "    local rel_retrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    local rel_notretrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    local irrel_retrived = {[1]=0,[2]=0,[3]=0,[4]=0,[5]=0}--,[6]=0,[7]=0}\n",
    "    for i=1,p_tags:size(1) do\n",
    "        for j=1,p_tags:size(2) do\n",
    "            for tag=1,5 do\n",
    "                if y_tags[i][j] == tag then\n",
    "                    rel_notretrived[tag] = rel_notretrived[tag] + 1\n",
    "                    if p_tags[i][j] == tag then\n",
    "                        rel_retrived[tag] = rel_retrived[tag] + 1\n",
    "                    else\n",
    "                        irrel_retrived[tag] = irrel_retrived[tag] + 1\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    local recall = 0\n",
    "    local precis = 0\n",
    "    for i=1,5 do\n",
    "        local r = (rel_retrived[i]/(rel_retrived[i]+rel_notretrived[i]))\n",
    "        local p = (rel_retrived[i]/(rel_retrived[i]+irrel_retrived[i]))\n",
    "        print(i, p, r, 2*p*r/(p+r))\n",
    "        recall = recall + r\n",
    "        precis = precis + p\n",
    "    end\n",
    "    return recall/5,precis/5\n",
    "end\n",
    "\n",
    "-- Writes to file in Kaggle prediction format\n",
    "-- p : prediction tensor\n",
    "-- fname : output filename\n",
    "function kaggle_format(p, fname)\n",
    "    local f = io.open(\"predictions/\" .. fname, \"w\")\n",
    "    f:write(\"ID,Labels\\n\")\n",
    "    for i=1,p:size(1) do\n",
    "        local s = \"\"\n",
    "        local t = {[1]=\"O\",[2]=\"PER\",[3]=\"LOC\",[4]=\"ORG\",[5]=\"MISC\",[6]=\"MISC\",[7]=\"LOC\",[8]=\"O\",[9]=\"O\"}\n",
    "        local prev_tag = 0\n",
    "        local prev_idx = 0\n",
    "        for j=2,p:size(2) do\n",
    "            if p[i][j] == 0 then\n",
    "                break\n",
    "            end\n",
    "            if p[i][j] ~= 1 then\n",
    "                if prev_tag == 0 then\n",
    "                    s = s .. t[p[i][j]] .. \"-\" .. tostring(j-1)\n",
    "                elseif p[i][j] == prev_tag and prev_idx == j-1 then\n",
    "                    s = s .. \"-\" .. tostring(j-1) \n",
    "                else\n",
    "                    s = s .. \" \" .. t[p[i][j]] .. \"-\" .. tostring(j-1)\n",
    "                end\n",
    "                prev_tag = p[i][j]\n",
    "                prev_idx = j\n",
    "            end\n",
    "        end\n",
    "        f:write(i .. \",\" .. s .. \"\\n\")\n",
    "    end\n",
    "    f:close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct, ce = hmm_fit(X_train, Y_train, 0.01, 0.01)\n",
    "p_tags = predict_tags(X_test_sen, ct, ce)\n",
    "--y_tags = format_valid(Y_valid)\n",
    "kaggle_format(p_tags, \"pred_\" .. tostring(os.time()) .. \"_hmm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t0.9545907279971\t0.4883839436685\t0.64617459125665\t\n",
       "2\t0.60221402214022\t0.37586365730078\t0.46284741917187\t\n",
       "3\t0.77259185700099\t0.43585434173669\t0.55730659025788\t\n",
       "4\t0.65711252653928\t0.39654067905189\t0.49460647223332\t\n",
       "5\t0.74559686888454\t0.42713004484305\t0.54312188168211\t\n",
       "0.42475453332018\t0.74642120051243\t\n",
       "0.54141454527312\t\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, p = predict_fscore(p_tags, y_tags)\n",
    "print(r, p)\n",
    "print(2*r*p/(r+p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
