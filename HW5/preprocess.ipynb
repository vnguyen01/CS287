{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import collections\n",
    "import numpy as np\n",
    "import operator\n",
    "import h5py\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import itertools\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "FILE_PATHS = {\"full\": (\"data/train.num.txt\",\n",
    "                      \"data/dev.num.txt\",\n",
    "                      \"data/dev_kaggle.txt\",\n",
    "                      \"data/tags.txt\",\n",
    "                      \"data/test.num.txt\",\n",
    "                      \"data/glove.txt\")}\n",
    "\n",
    "train_path, valid_path, valid_kaggle_path, tags_path, test_path, embeddings_path = FILE_PATHS[\"full\"]\n",
    "\n",
    "train = (list(csv.reader(open(train_path, 'rb'), delimiter='\\t', quotechar='|')))\n",
    "valid = (list(csv.reader(open(valid_path, 'rb'), delimiter='\\t',quotechar='|')))\n",
    "valid_kaggle = (list(csv.reader(open(valid_kaggle_path, 'rb'), delimiter='\\t', quotechar='|')))\n",
    "tags = (list(csv.reader(open(tags_path, 'rb'), delimiter=' ')))\n",
    "test = (list(csv.reader(open(test_path, 'rb'), delimiter=' ', quotechar='|')))\n",
    "\n",
    "dataset = [train, valid, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_float(c):\n",
    "    if c.isdigit():\n",
    "        return True\n",
    "    if c == ',':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def word_process(word):\n",
    "    #Realized that this can be done much easier with regular expresions...\n",
    "    #Removes digits and replaces them with the string 'NUMBER'\n",
    "    flag = 0\n",
    "    interval = [0,0]\n",
    "    intervals = []\n",
    "    processed = ''\n",
    "    if len(word) == 1 and (not word.isdigit()):\n",
    "        return word\n",
    "    for i in range(0,len(word)):\n",
    "        if flag == 0:\n",
    "            if is_float(word[i]):\n",
    "                flag = 1\n",
    "                interval[0] = i\n",
    "                #if len(word) == 1:\n",
    "                    #intervals.append((0,0))\n",
    "                if i == len(word)-1:\n",
    "                    intervals.append((interval[0], interval[0]+1))\n",
    "        else:\n",
    "            if (not is_float(word[i])):\n",
    "                flag = 0\n",
    "                interval[1] = i-1\n",
    "                intervals.append((interval[0],interval[1]))\n",
    "            elif i == len(word) -1:\n",
    "                interval[1] = i\n",
    "                intervals.append((interval[0],interval[1]))\n",
    "                \n",
    "    index = 0\n",
    "    for pair in intervals:\n",
    "        prefix = word[index:pair[0]]\n",
    "        index = pair[1]+1\n",
    "        processed = processed + prefix + 'NUMBER'\n",
    "    \n",
    "    processed = processed + word[index:]\n",
    "    return processed      \n",
    "\n",
    "def gen_dictionary(data_set, tag_set):\n",
    "    d = collections.defaultdict(list)\n",
    "    d['<s>'] = 1\n",
    "    d['</s>'] = 2\n",
    "    counter = 3\n",
    "    for data in data_set:\n",
    "        for element in data:\n",
    "            if element != []:\n",
    "                word = word_process(element[2])\n",
    "                if word not in d:\n",
    "                    d[word] = counter\n",
    "                    counter = counter + 1\n",
    "    for i in range(0, len(tag_set)):\n",
    "        tag_set[i][1] = int(tag_set[i][1])\n",
    "    tag_set = dict(tag_set)\n",
    "    tag_set['<s>'] = 8\n",
    "    tag_set['</s>'] = 9\n",
    "    \n",
    "    return d, tag_set\n",
    "\n",
    "def lex_features(data, v_dict):\n",
    "    lex = []\n",
    "    for i in range(0,len(data)):\n",
    "        row = data[i]\n",
    "        if row != []:\n",
    "            if int(row[1]) == 1:\n",
    "                lex.append(v_dict['<s>'])\n",
    "            lex.append(v_dict[word_process(row[2])])\n",
    "    return np.array(lex)\n",
    "\n",
    "def get_features(data, v_dict, t_dict, feature_functions):\n",
    "    inputs = feature_functions[0](data, v_dict)\n",
    "    for i in range(1, len(feature_functions)):\n",
    "        inputs = np.vstack((inputs, feature_functions[0](data,v_dict)))\n",
    "    return inputs\n",
    "        \n",
    "\n",
    "def get_outputs(data, t_dict):\n",
    "    outputs = []\n",
    "    for i in range(0, len(data)):\n",
    "        row = data[i]\n",
    "        if row != []:\n",
    "            if int(row[1]) == 1:\n",
    "                outputs.append(t_dict['<s>'])\n",
    "            outputs.append(t_dict[row[3]])\n",
    "    return np.array(outputs)\n",
    "\n",
    "def max_len(data):\n",
    "    maxlen = 0\n",
    "    count = 0\n",
    "    for i in range(0, len(data)):\n",
    "        row = data[i]\n",
    "        if row != []:\n",
    "            maxlen = max(int(row[1]), maxlen)\n",
    "            if int(row[1]) == 1:\n",
    "                count = count + 1\n",
    "    return maxlen, count\n",
    "\n",
    "def get_sentences(X_data, maxlen, numsen):\n",
    "    #takes in the index form of the X data, maximum sentence length, and number of sentences\n",
    "    sentences = np.zeros((numsen, maxlen+1))\n",
    "    sen_ind = -1\n",
    "    word_ind = 0\n",
    "    for ind in X_data:\n",
    "        if ind == 1:\n",
    "            sen_ind = sen_ind + 1\n",
    "            word_ind = 0\n",
    "        sentences[sen_ind, word_ind] = ind\n",
    "        word_ind = word_ind + 1\n",
    "    return sentences\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [lex_features]\n",
    "\n",
    "vocab_dict, tag_dict = gen_dictionary(dataset, tags)\n",
    "Y_train = get_outputs(train, tag_dict)\n",
    "Y_valid = get_outputs(valid, tag_dict)\n",
    "X_train = get_features(train, vocab_dict, tag_dict, features)\n",
    "X_valid = get_features(valid, vocab_dict, tag_dict, features)\n",
    "X_test = get_features(test, vocab_dict, tag_dict, features)\n",
    "\n",
    "#sentences\n",
    "max_length_valid, num_sentence_valid = max_len(valid)\n",
    "max_length_test, num_sentence_test = max_len(test)\n",
    "\n",
    "X_valid_sen = get_sentences(X_valid, max_length_valid, num_sentence_valid)\n",
    "X_test_sen = get_sentences(X_test, max_length_test, num_sentence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = max(vocab_dict.values())\n",
    "tag_size = max(tag_dict.values())\n",
    "nwords = np.max([np.max(X_train), np.max(X_valid), np.max(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'data' + '.hdf5'\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['X_train'] = X_train\n",
    "    f['Y_train'] = Y_train\n",
    "    f['X_valid'] = X_valid\n",
    "    f['Y_valid'] = Y_valid\n",
    "    f['X_test'] = X_test\n",
    "    f['nwords'] = np.array([nwords])\n",
    "    f['nclasses'] = np.array([tag_size])\n",
    "    f['nfeatures'] = np.array([len(features)])\n",
    "    f['X_valid_sen'] = X_valid_sen\n",
    "    f['X_test_sen'] = X_test_sen\n",
    "    f['X_valid_tag'] = X_valid_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "for i in xrange(2, len(valid_kaggle)):\n",
    "    tags.append((valid_kaggle[i][0].split(\",\"))[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid_tags = []\n",
    "for tag in tags:\n",
    "    temp = [\"\"]*max_length_valid\n",
    "    for j in xrange(len(tag)):\n",
    "        temp[j] = tag[j]\n",
    "    X_valid_tags.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid_tags = np.array(X_valid_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
