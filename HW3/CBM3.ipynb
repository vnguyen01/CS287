{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require('nn')\n",
    "require('hdf5')\n",
    "require('optim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = hdf5.open(\"data.hdf5\", \"r\")\n",
    "X_train = f:read(\"train_input\"):all()\n",
    "Y_train = f:read(\"train_output\"):all()\n",
    "X_valid = f:read(\"valid_input\"):all()\n",
    "q_valid = f:read(\"valid_q\"):all()\n",
    "X_test = f:read(\"test_input\"):all()\n",
    "q_test = f:read(\"test_q\"):all()\n",
    "kaggle_valid = f:read(\"valid_kaggle\"):all()\n",
    "nclasses = f:read('nclasses'):all():long()[1]\n",
    "nfeatures = f:read('nfeatures'):all():long()[1]\n",
    "f:close()\n",
    "\n",
    "sentences = 42068\n",
    "\n",
    "window_size = X_train:size(2)\n",
    "n_actual = nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function bigramCount(X, K)\n",
    "    local countMatrix = torch.zeros(K, K)\n",
    "    for i=1,X:size(1) do\n",
    "        for j=2,X:size(2) do\n",
    "            local prev_word = X[i][j-1]\n",
    "            local curr_word = X[i][j]\n",
    "            countMatrix[prev_word][curr_word] = countMatrix[prev_word][curr_word] + 1\n",
    "        end\n",
    "    end\n",
    "    return countMatrix\n",
    "end\n",
    "\n",
    "function unigramCount(X, Y, K)\n",
    "    local countVector = torch.zeros(1, K)\n",
    "    for i=1,X:size(1) do\n",
    "        for j=2,X:size(2) do\n",
    "            local idx = X[i][j]\n",
    "            countVector[1][idx] = countVector[1][idx] + 1\n",
    "        end\n",
    "    end\n",
    "    return countVector\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function additive(matrix, alpha)\n",
    "    local add_matrix = matrix:add(alpha)\n",
    "    return add_matrix\n",
    "end\n",
    "\n",
    "function normalize(CV, CM)\n",
    "    --divide by counts of previous word\n",
    "    local norm = torch.cdiv(CM,torch.expand(CV:t(), CM:size(1), CM:size(2)))\n",
    "    return norm\n",
    "end\n",
    "\n",
    "function bigramPerp(bcm, X, correct, q_50)\n",
    "    p = 0\n",
    "    for i=1,X:size(1) do\n",
    "        idx = q_50[i][correct[i]]\n",
    "        prev_word = X[i][5]\n",
    "        p = torch.log(bcm[prev_word][idx]) + p\n",
    "    end\n",
    "    return torch.exp(-p/X_valid:size(1))\n",
    "end\n",
    "\n",
    "function kaggle(bcm, test, q)\n",
    "    scores = torch.zeros(q:size(1), 50)\n",
    "    for i=1,test:size(1) do\n",
    "        prev = test[i][5]\n",
    "        for j=1,50 do\n",
    "            w = q[i][j]\n",
    "            scores[i][j] = bcm[prev][w]\n",
    "        end\n",
    "    end\n",
    "    return scores:cdiv(torch.expand((torch.sum(scores, 2)), scores:size(1), scores:size(2)))\n",
    "end\n",
    "\n",
    "function write2file(scores, fname)\n",
    "    f = io.open(fname, \"w\")\n",
    "    f:write(\"ID,Class1,CLass2,Class3,Class4,Class5,Class6,Class7,Class8,Class9,Class10,Class11,Class12,CLass13,Class14,Class15,Class16,Class17,Class18,Class19,Class20,Class21,Class22,Class23,CLass24,Class25,Class26,Class27,Class28,Class29,Class30,Class31,Class32,Class33,Class34,CLass35,Class36,Class37,Class38,Class39,Class40,Class41,Class42,Class43,Class44,Class45,CLass46,Class47,Class48,Class49,Class50\\n\")\n",
    "    for i=1,scores:size(1) do\n",
    "        s = tostring(i)\n",
    "        for j=1, scores:size(2) do\n",
    "            s = s .. \",\" .. tostring(scores[i][j])\n",
    "        end\n",
    "        f:write(s .. \"\\n\")\n",
    "    end\n",
    "    f:close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CM = bigramCount(X_train, nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV = unigramCount(X_train, Y_train, nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BCM = normalize(CV, CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = kaggle(BCM, X_test, q_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write2file(scores, \"asdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268.7793631882\t\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramPerp(bcm, X_valid, kaggle_valid, q_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bcm = torch.cdiv(BCM, torch.expand(BCM:sum(2), BCM:size(1), BCM:size(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999975\t\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcm:sub(1,1):sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     3      3      3   9981  10001\n",
       "     3      3   9981  10001   9987\n",
       "     3   9981  10001   9987   9978\n",
       "  9981  10001   9987   9978   9991\n",
       " 10001   9987   9978   9991   9975\n",
       "  9987   9978   9991   9975   9985\n",
       "  9978   9991   9975   9985   9977\n",
       "  9991   9975   9985   9977   9993\n",
       "  9975   9985   9977   9993   9972\n",
       "  9985   9977   9993   9972   9973\n",
       "  9977   9993   9972   9973   9989\n",
       "  9993   9972   9973   9989   9997\n",
       "  9972   9973   9989   9997   9979\n",
       "  9973   9989   9997   9979   9983\n",
       "  9989   9997   9979   9983   9974\n",
       "  9997   9979   9983   9974   9984\n",
       "  9979   9983   9974   9984   9990\n",
       "  9983   9974   9984   9990   9976\n",
       "  9974   9984   9990   9976  10000\n",
       "  9984   9990   9976  10000   9994\n",
       "  9990   9976  10000   9994   9995\n",
       "  9976  10000   9994   9995   9998\n",
       " 10000   9994   9995   9998   9986\n",
       "     3      3      3   9000      2\n",
       "     3      3   9000      2      5\n",
       "     3   9000      2      5     74\n",
       "  9000      2      5     74    395\n",
       "     2      5     74    395     35\n",
       "     5     74    395     35   2150\n",
       "    74    395     35   2150      1\n",
       "   395     35   2150      1    148\n",
       "    35   2150      1    148     21\n",
       "  2150      1    148     21      8\n",
       "     1    148     21      8   9477\n",
       "   148     21      8   9477    277\n",
       "    21      8   9477    277    409\n",
       "     8   9477    277    409      5\n",
       "     3      3      3     25      2\n",
       "     3      3     25      2     15\n",
       "     3     25      2     15    144\n",
       "    25      2     15    144      6\n",
       "     2     15    144      6      2\n",
       "    15    144      6      2   5461\n",
       "   144      6      2   5461      1\n",
       "     6      2   5461      1   3108\n",
       "     2   5461      1   3108   1585\n",
       "  5461      1   3108   1585     98\n",
       "     3      3      3   7254      2\n",
       "     3      3   7254      2      5\n",
       "     3   7254      2      5     74\n",
       "[torch.LongTensor of size 50x5]\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train:sub(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_sen_idx = torch.zeros(1, 41931)\n",
    "sen_count = 0\n",
    "for i=1,X_train:size(1) do\n",
    "    row = X_train[i]\n",
    "    if (row[1] ==row[2] and row[2]==row[3] and row[1]==3) then\n",
    "        sen_count = sen_count + 1\n",
    "        new_sen_idx[1][sen_count] = i \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sen_counter = 2\n",
    "i = 1\n",
    "sen_len = 0\n",
    "c = 0\n",
    "a = 0\n",
    "sen_matrix = torch.zeros(sen_count, 85)\n",
    "while i < X_train:size(1) do\n",
    "    if i < new_sen_idx[1][sen_counter] then\n",
    "        for j=1,5 do\n",
    "            c = c + 1\n",
    "            sen_matrix[sen_counter-1][c]= X_train[i][j]\n",
    "            --print(test[i][j])\n",
    "        end\n",
    "        i = i + 5\n",
    "    else\n",
    "        last_win = new_sen_idx[1][sen_counter]-1\n",
    "        mid_sen = (2+5-(new_sen_idx[1][sen_counter]-(i-5)))\n",
    "        for j=mid_sen,5 do\n",
    "            c = c + 1\n",
    "            --print(c)\n",
    "            sen_matrix[sen_counter-1][c] = X_train[last_win][j]\n",
    "            --print(test[last_win][j])\n",
    "        end\n",
    "        if c > sen_len then\n",
    "            sen_len = c\n",
    "        end\n",
    "        --print(sen_matrix:sub(sen_counter-1, sen_counter-1))\n",
    "        --print(sen_counter,c)\n",
    "        c = 0\n",
    "        i = new_sen_idx[1][sen_counter]\n",
    "        sen_counter = sen_counter + 1\n",
    "        if sen_counter > new_sen_idx:size(2) then\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 13\n",
       "    3     3     3     2    82     1   169     6    37  2646     2    67    12\n",
       "\n",
       "Columns 14 to 26\n",
       "  560  6206  3558  1886   666     2     9    29     2  4396  6143     9     5\n",
       "\n",
       "Columns 27 to 39\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 40 to 52\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 53 to 65\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 66 to 78\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 79 to 85\n",
       "    0     0     0     0     0     0     0\n",
       "[torch.DoubleTensor of size 1x85]\n",
       "\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_matrix:sub(7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460\t\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     3      3      3   9981  10001\n",
       "     3      3   9981  10001   9987\n",
       "     3   9981  10001   9987   9978\n",
       "  9981  10001   9987   9978   9991\n",
       " 10001   9987   9978   9991   9975\n",
       "  9987   9978   9991   9975   9985\n",
       "  9978   9991   9975   9985   9977\n",
       "  9991   9975   9985   9977   9993\n",
       "  9975   9985   9977   9993   9972\n",
       "  9985   9977   9993   9972   9973\n",
       "  9977   9993   9972   9973   9989\n",
       "  9993   9972   9973   9989   9997\n",
       "  9972   9973   9989   9997   9979\n",
       "  9973   9989   9997   9979   9983\n",
       "  9989   9997   9979   9983   9974\n",
       "  9997   9979   9983   9974   9984\n",
       "  9979   9983   9974   9984   9990\n",
       "  9983   9974   9984   9990   9976\n",
       "  9974   9984   9990   9976  10000\n",
       "  9984   9990   9976  10000   9994\n",
       "  9990   9976  10000   9994   9995\n",
       "  9976  10000   9994   9995   9998\n",
       " 10000   9994   9995   9998   9986\n",
       "     3      3      3   9000      2\n",
       "     3      3   9000      2      5\n",
       "     3   9000      2      5     74\n",
       "  9000      2      5     74    395\n",
       "     2      5     74    395     35\n",
       "     5     74    395     35   2150\n",
       "    74    395     35   2150      1\n",
       "   395     35   2150      1    148\n",
       "    35   2150      1    148     21\n",
       "  2150      1    148     21      8\n",
       "     1    148     21      8   9477\n",
       "   148     21      8   9477    277\n",
       "    21      8   9477    277    409\n",
       "     8   9477    277    409      5\n",
       "     3      3      3     25      2\n",
       "     3      3     25      2     15\n",
       "     3     25      2     15    144\n",
       "    25      2     15    144      6\n",
       "     2     15    144      6      2\n",
       "    15    144      6      2   5461\n",
       "   144      6      2   5461      1\n",
       "     6      2   5461      1   3108\n",
       "     2   5461      1   3108   1585\n",
       "  5461      1   3108   1585     98\n",
       "     3      3      3   7254      2\n",
       "     3      3   7254      2      5\n",
       "     3   7254      2      5     74\n",
       "  7254      2      5     74    395\n",
       "     2      5     74    395     10\n",
       "     5     74    395     10    339\n",
       "    74    395     10    339    144\n",
       "   395     10    339    144      6\n",
       "    10    339    144      6   2483\n",
       "   339    144      6   2483    661\n",
       "   144      6   2483    661   2187\n",
       "     6   2483    661   2187    958\n",
       "  2483    661   2187    958     26\n",
       "   661   2187    958     26    524\n",
       "  2187    958     26    524      8\n",
       "   958     26    524      8   9477\n",
       "    26    524      8   9477    277\n",
       "   524      8   9477    277      6\n",
       "     8   9477    277      6     41\n",
       "  9477    277      6     41    305\n",
       "   277      6     41    305    440\n",
       "     6     41    305    440   3690\n",
       "     3      3      3      8    946\n",
       "     3      3      8    946      6\n",
       "     3      8    946      6   3222\n",
       "     8    946      6   3222    501\n",
       "   946      6   3222    501    264\n",
       "     6   3222    501    264      7\n",
       "  3222    501    264      7    140\n",
       "   501    264      7    140   6206\n",
       "   264      7    140   6206   4396\n",
       "     7    140   6206   4396   6143\n",
       "   140   6206   4396   6143     32\n",
       "  6206   4396   6143     32    990\n",
       "  4396   6143     32    990      8\n",
       "  6143     32    990      8    242\n",
       "    32    990      8    242    762\n",
       "   990      8    242    762      6\n",
       "     8    242    762      6   1020\n",
       "   242    762      6   1020   2819\n",
       "   762      6   1020   2819    213\n",
       "     6   1020   2819    213      8\n",
       "  1020   2819    213      8     98\n",
       "  2819    213      8     98      6\n",
       "   213      8     98      6    433\n",
       "     8     98      6    433   4126\n",
       "    98      6    433   4126      7\n",
       "     6    433   4126      7     16\n",
       "   433   4126      7     16     47\n",
       "  4126      7     16     47     57\n",
       "     7     16     47     57      5\n",
       "    16     47     57      5     74\n",
       "    47     57      5     74    197\n",
       "    57      5     74    197   1249\n",
       "     5     74    197   1249    222\n",
       "     3      3      3      1   3222\n",
       "     3      3      1   3222   7770\n",
       "     3      1   3222   7770      2\n",
       "     1   3222   7770      2     15\n",
       "  3222   7770      2     15   4039\n",
       "  7770      2     15   4039      2\n",
       "     2     15   4039      2    501\n",
       "    15   4039      2    501     16\n",
       "  4039      2    501     16   6742\n",
       "     2    501     16   6742      1\n",
       "   501     16   6742      1      2\n",
       "    16   6742      1      2     24\n",
       "  6742      1      2     24    115\n",
       "     1      2     24    115   2701\n",
       "     2     24    115   2701   8204\n",
       " "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   24    115   2701   8204      7\n",
       "   115   2701   8204      7     16\n",
       "  2701   8204      7     16   2471\n",
       "  8204      7     16   2471   5076\n",
       "     7     16   2471   5076     12\n",
       "    16   2471   5076     12    466\n",
       "  2471   5076     12    466     54\n",
       "  5076     12    466     54   2986\n",
       "    12    466     54   2986    468\n",
       "   466     54   2986    468   1249\n",
       "    54   2986    468   1249     17\n",
       "     3      3      3      2     82\n",
       "     3      3      2     82      1\n",
       "     3      2     82      1    169\n",
       "     2     82      1    169      6\n",
       "    82      1    169      6     37\n",
       "     1    169      6     37   2646\n",
       "   169      6     37   2646      2\n",
       "     6     37   2646      2     67\n",
       "    37   2646      2     67     12\n",
       "  2646      2     67     12    560\n",
       "     2     67     12    560   6206\n",
       "    67     12    560   6206   3558\n",
       "    12    560   6206   3558   1886\n",
       "   560   6206   3558   1886    666\n",
       "  6206   3558   1886    666      2\n",
       "  3558   1886    666      2      9\n",
       "  1886    666      2      9     29\n",
       "   666      2      9     29      2\n",
       "     2      9     29      2   4396\n",
       "     9     29      2   4396   6143\n",
       "    29      2   4396   6143      9\n",
       "     2   4396   6143      9      5\n",
       "     3      3      3    369   1981\n",
       "     3      3    369   1981   3150\n",
       "     3    369   1981   3150     48\n",
       "   369   1981   3150     48    222\n",
       "  1981   3150     48    222     47\n",
       "  3150     48    222     47     57\n",
       "    48    222     47     57      8\n",
       "   222     47     57      8     42\n",
       "    47     57      8     42    197\n",
       "    57      8     42    197      1\n",
       "     8     42    197      1    467\n",
       "    42    197      1    467    344\n",
       "   197      1    467    344   1300\n",
       "     1    467    344   1300      9\n",
       "   467    344   1300      9    327\n",
       "   344   1300      9    327     11\n",
       "  1300      9    327     11     37\n",
       "     9    327     11     37   1493\n",
       "   327     11     37   1493    920\n",
       "    11     37   1493    920      6\n",
       "    37   1493    920      6   3176\n",
       "  1493    920      6   3176      8\n",
       "   920      6   3176      8   9343\n",
       "     6   3176      8   9343    373\n",
       "  3176      8   9343    373      7\n",
       "     8   9343    373      7   1145\n",
       "  9343    373      7   1145     37\n",
       "   373      7   1145     37   1421\n",
       "     7   1145     37   1421      7\n",
       "  1145     37   1421      7      1\n",
       "    37   1421      7      1    434\n",
       "     3      3      3      8      2\n",
       "     3      3      8      2      2\n",
       "     3      8      2      2     17\n",
       "     8      2      2     17     41\n",
       "     2      2     17     41     15\n",
       "     2     17     41     15     33\n",
       "    17     41     15     33    395\n",
       "    41     15     33    395   1364\n",
       "     3      3      3     66    279\n",
       "     3      3     66    279   1924\n",
       "     3     66    279   1924     45\n",
       "    66    279   1924     45     74\n",
       "   279   1924     45     74    197\n",
       "  1924     45     74    197    159\n",
       "    45     74    197    159   1454\n",
       "    74    197    159   1454   2379\n",
       "   197    159   1454   2379      6\n",
       "   159   1454   2379      6   3222\n",
       "  1454   2379      6   3222    717\n",
       "[torch.LongTensor of size 200x5]\n",
       "\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
