{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import argparse\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Different data sets to try.\n",
    "# Note: TREC has no development set.\n",
    "# And SUBJ and MPQA have no splits (must use cross-validation)\n",
    "FILE_PATHS = {\"SST1\": (\"data/stsa.fine.phrases.train\",\n",
    "                       \"data/stsa.fine.dev\",\n",
    "                       \"data/stsa.fine.test\"),\n",
    "              \"SST2\": (\"data/stsa.binary.phrases.train\",\n",
    "                       \"data/stsa.binary.dev\",\n",
    "                       \"data/stsa.binary.test\"),\n",
    "              \"TREC\": (\"data/TREC.train.all\", None,\n",
    "                       \"data/TREC.test.all\"),\n",
    "              \"SUBJ\": (\"data/subj.all\", None, None),\n",
    "              \"MPQA\": (\"data/mpqa.all\", None, None)}\n",
    "\n",
    "# Kaggle dataset for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for the SST dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_to_words(line, dataset):\n",
    "    # Different preprocessing is used for these datasets.\n",
    "    if dataset not in ['SST1', 'SST2']:\n",
    "        clean_line = clean_str_sst(line.strip())\n",
    "    else:\n",
    "        clean_line = clean_str(line.strip())\n",
    "    clean_line = clean_str_sst(line.strip())\n",
    "    words = clean_line.split(' ')\n",
    "    words = words[1:]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(file_list, dataset=''):\n",
    "    \"\"\"\n",
    "    Construct index feature dictionary.\n",
    "    EXTENSION: Change to allow for other word features, or bigrams.\n",
    "    \"\"\"\n",
    "    max_sent_len = 0\n",
    "    word_to_idx = {}\n",
    "    # Start at 2 (1 is padding)\n",
    "    idx = 2\n",
    "    for filename in file_list:\n",
    "        if filename:\n",
    "            with open(filename, \"r\") as f:\n",
    "                for line in f:\n",
    "                    words = line_to_words(line, dataset)\n",
    "                    max_sent_len = max(max_sent_len, len(words))\n",
    "                    for word in words:\n",
    "                        if word not in word_to_idx:\n",
    "                            word_to_idx[word] = idx\n",
    "                            idx += 1\n",
    "    return max_sent_len, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data(data_name, word_to_idx, max_sent_len, dataset, start_padding=0):\n",
    "    \"\"\"\n",
    "    Convert data to padded word index features.\n",
    "    EXTENSION: Change to allow for other word features, or bigrams.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    lbl = []\n",
    "    with open(data_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line_to_words(line, dataset)\n",
    "            y = int(line[0]) + 1\n",
    "            sent = [word_to_idx[word] for word in words]\n",
    "            sent = list(set(sent))\n",
    "            # end padding\n",
    "            if len(sent) < max_sent_len + start_padding:\n",
    "                sent.extend([1] * (max_sent_len + start_padding - len(sent)))\n",
    "            # start padding\n",
    "            sent = [1]*start_padding + sent\n",
    "            features.append(sent)\n",
    "            lbl.append(y)\n",
    "    return np.array(features, dtype=np.int32), np.array(lbl, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(arguments):\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument('dataset', help=\"Data set\",\n",
    "                        type=str)\n",
    "    args = parser.parse_args(arguments)\n",
    "    dataset = args.dataset\n",
    "    train, valid, test = FILE_PATHS[dataset]\n",
    "    \n",
    "    # Features are just the words.\n",
    "    max_sent_len, word_to_idx = get_vocab([train, valid, test])\n",
    "    \n",
    "    # Dataset name\n",
    "    train_input, train_output = convert_data(train, word_to_idx, max_sent_len,\n",
    "                                             dataset)\n",
    "    \n",
    "    if valid:\n",
    "        valid_input, valid_output = convert_data(valid, word_to_idx, max_sent_len,\n",
    "                                                 dataset)\n",
    "\n",
    "    if test:\n",
    "        test_input, _ = convert_data(test, word_to_idx, max_sent_len,\n",
    "                                 dataset)\n",
    "\n",
    "\n",
    "    V = len(word_to_idx) + 1\n",
    "    print('Vocab size:', V)\n",
    "    \n",
    "    C = np.max(train_output)\n",
    "\n",
    "    filename = args.dataset + '.hdf5'\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        print(valid_output.shape)\n",
    "        f['train_input'] = train_input\n",
    "        f['train_output'] = train_output\n",
    "        if valid:\n",
    "            f['valid_input'] = valid_input\n",
    "            f['valid_output'] = valid_output\n",
    "        if test:\n",
    "            f['test_input'] = test_input\n",
    "        f['nfeatures'] = np.array([V], dtype=np.int32)\n",
    "        f['nclasses'] = np.array([C], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocab size:', 17837)\n",
      "(1101,)\n"
     ]
    }
   ],
   "source": [
    "args = {\"SST1\"}\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
