{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(\"hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multinomial Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function logsumexp(z)\n",
    "    --Log Sum Exp Trick \n",
    "        --https://hips.seas.harvard.edu/blog/2013/01/09/computing-log-sum-exp/\n",
    "        --Let a = max_n (XW^T+b)_n\n",
    "        --so that\n",
    "        --a + \\log \\sum \\exp (XW^T+b - a) \n",
    "    --find the maximum values in each column\n",
    "    local a = z:max(2)\n",
    "    --subtract constant from XW^T+b\n",
    "    z:csub(torch.expand(a,a:size(1), z:size(2)))   \n",
    "    z:exp()\n",
    "    z = z:sum(2)\n",
    "    z:log()\n",
    "    --add constant back in\n",
    "    z:add(a)\n",
    "    return z\n",
    "end\n",
    "\n",
    "function loss_minimization(W, X, Y, lambda)\n",
    "    W = W:reshape(Y:size(2), X:size(2)+1)\n",
    "    --intercept\n",
    "    local b = W:sub(1, W:size(1), W:size(2),W:size(2)):t()\n",
    "    --coefficients\n",
    "    W = W:sub(1, W:size(1),1,W:size(2)-1)\n",
    "    --z_c = XW^T + b\n",
    "    local z = (X*W:t()):add(b:expand(b,X:size(1),b:size(2)))\n",
    "    --\\log \\sum \\exp z_c\n",
    "    z_c = logsumexp(z:clone())\n",
    "    --z - \\log \\sum \\exp z_c \n",
    "    z:csub(torch.expand(z_c, z_c:size(1), z:size(2)))\n",
    "    --L2 regularization\n",
    "    local norm = W:reshape(W:size(1)*W:size(2), 1)\n",
    "    --L1 regularization\n",
    "    --torch.sum(W) --put that above return \n",
    "    --Cross Entropy Loss\n",
    "    local loss = (torch.sum(torch.cmul(Y,z))*-1) + 0.5 * lambda * torch.sum(W)--torch.dot(norm, norm)\n",
    "    return loss, z:exp(), W\n",
    "end\n",
    "\n",
    "function minibatch(X, Y, bsize)\n",
    "    --random ordering of ints [1,nexamples] and take first bsize\n",
    "    local idx = torch.randperm(X:size(1)):sub(1,bsize)\n",
    "    --training minibatches\n",
    "    local X_batch = torch.Tensor(bsize, X:size(2))\n",
    "    local Y_batch = torch.Tensor(bsize, Y:size(2))\n",
    "    for i=1,bsize do\n",
    "        X_batch[i] = X[idx[i]]\n",
    "        Y_batch[i] = Y[idx[i]]\n",
    "    end\n",
    "    return X_batch, Y_batch\n",
    "end\n",
    "\n",
    "function grad_loss_minimization(W, X, Y, bsize, lambda)\n",
    "    --do minibatch sampling\n",
    "    local X_batch, Y_batch = minibatch(X, Y, bsize)\n",
    "    local loss, mu, W = loss_minimization(W, X_batch, Y_batch, lambda)\n",
    "    \n",
    "    --calculate the gradient\n",
    "    --g(W) = \\sum (\\mu_i - y_i) \\times x_i\n",
    "    --from Murphy pg. 253\n",
    "    local mu_y = torch.csub(mu, Y_batch)\n",
    "    local grad = mu_y:t()*X_batch\n",
    "    grad:add(W)\n",
    "    grad = grad:cat(torch.zeros(grad:size(1),1), 2)\n",
    "    grad:sub(1, grad:size(1), grad:size(2), grad:size(2)):add(mu_y:sum(1))\n",
    "    neval = neval + 1\n",
    "    print(neval, loss)\n",
    "    return grad:reshape(grad:size(1)*grad:size(2), 1)\n",
    "end\n",
    "\n",
    "function fit(X, Y, bsize, rate, iter, lambda)\n",
    "    --Weight matrix must be passed in as vector\n",
    "    local W = torch.zeros(Y:size(2) * (X:size(2)+1), 1)\n",
    "\n",
    "    \n",
    "    --params\n",
    "    local lr = rate\n",
    "    local b1 = 0.9\n",
    "    local b2 = 0.999\n",
    "    local e = 1e-8\n",
    "    local t = 0\n",
    "    local m\n",
    "    local v\n",
    "    local denom\n",
    "\n",
    "    function adam(W)\n",
    "        --quicker and smoother than sgd\n",
    "        --https://github.com/torch/optim/blob/master/adam.lua\n",
    "        --http://arxiv.org/pdf/1412.6980.pdf\n",
    "        local grad = grad_loss_minimization(W, X, Y, bsize, lambda)\n",
    "        m = m or W.new(grad:size()):zero()\n",
    "        v = v or W.new(grad:size()):zero()\n",
    "        denom = denom or W.new(grad:size()):zero()\n",
    "        t = t + 1\n",
    "        m:mul(b1):add(1-b1, grad)\n",
    "        v:mul(b2):addcmul(1-b2, grad, grad)\n",
    "        denom:copy(v):sqrt():add(e)\n",
    "        local biasCorrection1 = 1 - b1^t\n",
    "        local biasCorrection2 = 1 - b2^t\n",
    "        local stepSize = lr * math.sqrt(biasCorrection2)/biasCorrection1\n",
    "        W:addcdiv(-stepSize, m, denom)\n",
    "        return W\n",
    "    end\n",
    "    \n",
    "    --[[\n",
    "    function sgd(W)\n",
    "        local grad = grad_loss_minimization(W, X, Y, bsize)\n",
    "        grad:mul(lr)\n",
    "        W:csub(grad)\n",
    "        return W\n",
    "    end\n",
    "    ]]\n",
    "    \n",
    "    for i=1,iter do\n",
    "        --W = sgd(W)\n",
    "        W = adam(W)\n",
    "    end\n",
    "\n",
    "    W = W:reshape(Y:size(2), X:size(2)+1)\n",
    "    --intercept\n",
    "    b = W:sub(1, W:size(1), W:size(2), W:size(2))\n",
    "    --coefficients\n",
    "    W = W:sub(1, W:size(1), 1, W:size(2)-1)\n",
    "    return W, b\n",
    "end\n",
    "\n",
    "function predict(X, W, b)\n",
    "    local b = b:t()\n",
    "    return (X*W:t()):add(b:expand(b, X:size(1), b:size(2)))\n",
    "end\n",
    "\n",
    "function predict_score(ypred, ytrue)\n",
    "    local c = 0\n",
    "    for i=1,ypred:size(1) do\n",
    "        if ypred[i][1] == ytrue[i][1] then\n",
    "            c = c + 1       \n",
    "        end\n",
    "    end\n",
    "    return c/ypred:size(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--feature weight: counts\n",
    "function createDocWordMatrix(vocab, max_sent_len, sparseMatrix)\n",
    "    docword = torch.zeros(sparseMatrix:size(1), vocab)\n",
    "    for i=1,sparseMatrix:size(1) do\n",
    "        for j=1, max_sent_len do\n",
    "            local idx = (sparseMatrix[i][j])\n",
    "            if idx ~= 0 then\n",
    "                docword[i][idx] = 1 --+ docword[i][idx]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return docword\n",
    "end\n",
    " \n",
    "function onehotencode(classes, target)\n",
    "    onehot = torch.zeros(target:size(1), classes)\n",
    "    for i=1,target:size(1) do\n",
    "        onehot[i][target[i]] = 1\n",
    "    end\n",
    "    return onehot\n",
    "end\n",
    "\n",
    "function write2file(fname, pred) \n",
    "    f = io.open(fname, \"w\")\n",
    "    f:write(\"ID,Category\\n\")\n",
    "    for i=1,pred:size(1) do\n",
    "        f:write(tostring(i) .. \",\" .. tostring(pred[i][1]) .. \"\\n\")\n",
    "    end\n",
    "    f:close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = hdf5.open(\"SST1.hdf5\", \"r\")\n",
    "\n",
    "X_train = f:read(\"train_input\"):all()\n",
    "Y_train = f:read(\"train_output\"):all()\n",
    "X_valid = f:read(\"valid_input\"):all()\n",
    "Y_valid = f:read(\"valid_output\"):all()\n",
    "--X_test = f:read(\"test_input\"):all()\n",
    "nclasses = f:read('nclasses'):all():long()[1]\n",
    "nfeatures = f:read('nfeatures'):all():long()[1]\n",
    "\n",
    "f:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train =createDocWordMatrix(nfeatures, 53, X_train)\n",
    "Y_train = onehotencode(nclasses, Y_train)\n",
    "--X_test = createDocWordMatrix(nfeatures, 53, X_valid)\n",
    "--Y_test = onehotencode(nclasses, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t16094.379124338\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3\t\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = os.time()\n",
    "--input params\n",
    "    --input features\n",
    "    --target one-hot encodes\n",
    "    --batch size\n",
    "    --learning rate\n",
    "    --max iterations\n",
    "    --lambda\n",
    "neval=0\n",
    "W, b = fit(X_train, Y_train, 10000, 0.1, 1, 1)\n",
    "end_time = os.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90373046088278\t\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = predict(X_train, W, b)\n",
    "_, Y_pred = torch.max(Y_pred, 2)\n",
    "_,Y_true = torch.max(Y_train, 2)\n",
    "acc_score = predict_score(Y_pred, Y_true)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write2file(\"LRL2_SST2.csv\", Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
