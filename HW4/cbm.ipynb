{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'hdf5'\n",
    "require 'math'\n",
    "\n",
    "f = hdf5.open(\"data.hdf5\", \"r\")\n",
    "\n",
    "X_train = f:read(\"windows_train\"):all()\n",
    "Y_train = f:read(\"train_Y\"):all()\n",
    "\n",
    "X_valid = f:read(\"windows_valid\"):all()\n",
    "X_valid_nospaces = f:read(\"valid_kaggle_without_spaces\"):all()\n",
    "\n",
    "Y_valid = f:read(\"valid_reduced_Y\"):all()\n",
    "Y_valid_spaces = f:read(\"valid_answers\"):all()\n",
    "\n",
    "nclasses = f:read('nclasses'):all():long()[1]\n",
    "nfeatures = f:read('nfeatures'):all():long()[1]\n",
    "\n",
    "f:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--Returns converted LongTensor to Str split by space row-wise\n",
    "function createHash(longTensor) \n",
    "    local s = \"\"\n",
    "    for i=1,longTensor:size(1) do\n",
    "        s = s .. tostring(longTensor[i]) .. \" \"\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "--Returns converted LongTensor to Str split by space col-wise\n",
    "function createHash2(longTensor)\n",
    "    local s = \"\"\n",
    "    for i=1,longTensor:size(2) do\n",
    "        s = s .. tostring(longTensor[1][i]) .. \" \"\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "--Return added padding when necessary at </s>\n",
    "function add_padding(r, n, win) \n",
    "    if r < n then\n",
    "        for i=1,n-r do\n",
    "            win[i] = 30\n",
    "        end\n",
    "        r = r + 1\n",
    "    end\n",
    "    if win[n] == 30 then\n",
    "        r = 1\n",
    "    end\n",
    "    return r, win\n",
    "end\n",
    "\n",
    "--Returns table of space and total counts\n",
    "function count_train(windows_train, space_train, n_gram)\n",
    "    local count_table = {}\n",
    "    local space_table = {}\n",
    "    local restart = 1\n",
    "    for i=1,windows_train:size(1) do\n",
    "        --Checks if a </s> is necessary (only for CBM!)\n",
    "        restart, padded = add_padding(restart, n_gram, windows_train[i])\n",
    "        local key = createHash(padded)\n",
    "        if count_table[key] then\n",
    "            count_table[key] = count_table[key] + 1\n",
    "        else\n",
    "            count_table[key] = 1\n",
    "            space_table[key] = 0\n",
    "        end\n",
    "        if space_train[i] == 2 then\n",
    "            space_table[key] = space_table[key] + 1\n",
    "        end\n",
    "    end\n",
    "    return space_table, count_table\n",
    "end  \n",
    "\n",
    "--Returns normalized probability for two scores\n",
    "function normalize(num, den) \n",
    "    return num/(num+den)\n",
    "end\n",
    "\n",
    "--Returns smoothed counts\n",
    "function laplace_smooth(count, total, alpha, vocab_size)\n",
    "    return (count+alpha)/(total+alpha*vocab_size)\n",
    "end\n",
    "\n",
    "--Returns perplexity for CBM\n",
    "function count_perp(space_table, total_table, windows_valid, space_valid, n_gram, alpha, vocab_size)\n",
    "    local restart = 1\n",
    "    local perp = 0\n",
    "    for i=1,windows_valid:size(1) do\n",
    "        --Checks if a </s> is necessary (only for CBM!)\n",
    "        restart, padded = add_padding(restart, n_gram, windows_valid[i])\n",
    "        local key = createHash(padded)\n",
    "        if total_table[key] then\n",
    "            local p_space = laplace_smooth(space_table[key], total_table[key], alpha, vocab_size)\n",
    "            local p_nospace = laplace_smooth(total_table[key] - space_table[key], total_table[key], alpha, vocab_size)\n",
    "            \n",
    "            --Next char is NOT space\n",
    "            if space_valid[i] == 1 then\n",
    "                perp = perp + math.log(normalize(p_nospace, p_space))\n",
    "            --Next char IS space\n",
    "            else\n",
    "                perp = perp + math.log(normalize(p_space, p_nospace))\n",
    "            end\n",
    "        else\n",
    "            --Probability for unseen counts\n",
    "            perp = perp + math.log(alpha/vocab_size)\n",
    "        end\n",
    "    end\n",
    "    return math.exp(-perp/windows_valid:size(1))\n",
    "end\n",
    "\n",
    "--Returns array of perplexity scores for different alphas\n",
    "function count_based_CV(space_table, total_table, x_valid, y_valid, w, a, n)\n",
    "    local list=\"\"\n",
    "    for i=1,100 do\n",
    "        local a = i/10\n",
    "        local perplexity = count_perp(space_table, total_table, x_valid, y_valid, w, a, n)\n",
    "        list=list..tostring(perplexity)..\",\"\n",
    "    end\n",
    "    return list\n",
    "end\n",
    "\n",
    "--Returns padded X_valid_nospaces for space count\n",
    "function pre_pad(x_valid_nospaces, w)\n",
    "    local padding = torch.Tensor(x_valid_nospaces:size(1), w-1):fill(30)\n",
    "    padding = padding:type('torch.LongTensor')\n",
    "    return torch.cat(padding, x_valid_nospaces, 2)\n",
    "end\n",
    "\n",
    "--Returns MSE of spaces for GREEDY CBM given following parameters:\n",
    "    --space_table : hash_table of ngram : n_spaces\n",
    "    --total_table : hash_table of ngram : counts\n",
    "    --x_valid : sentences with NO spaces\n",
    "    --y_valid : number of spaces in each sentence\n",
    "    --w : window size\n",
    "    --n : nfeatures\n",
    "    --a : additive alpha\n",
    "    --p : threshold paramters\n",
    "function count_greedy_predict(space_table, total_table, x_valid, y_valid, w, n, a, p)\n",
    "    local mean_error_sq = 0\n",
    "    for i=1,x_valid:size(1) do\n",
    "        local spaces = 0\n",
    "        local sentence = x_valid:sub(i,i,1,x_valid:size(2)):clone()\n",
    "        for j=1,x_valid:size(2)-w+1 do\n",
    "            if sentence[1][j+w] == 30 then\n",
    "                break\n",
    "            end\n",
    "            local context = sentence:sub(1,1,j,j+w-1)\n",
    "            local key = createHash2(context)\n",
    "            local p_space = 0\n",
    "            local p_nospace = 0\n",
    "            if total_table[key] then\n",
    "                p_space = laplace_smooth(space_table[key], total_table[key], a, n)\n",
    "                p_nospace = laplace_smooth(total_table[key] - space_table[key], total_table[key], a, n)\n",
    "                p_space = normalize(p_space, p_nospace)\n",
    "                ----print(p_space)\n",
    "            else\n",
    "                p_space = 1/n\n",
    "            end\n",
    "            if p_space > p then\n",
    "                spaces = spaces + 1\n",
    "                --Loop through backwards from the second to last entry up to position of next space\n",
    "                for k=1,sentence:size(2)-j-w do\n",
    "                    local idx = sentence:size(2) - k\n",
    "                    sentence[1][idx+1] = sentence[1][idx]\n",
    "                end\n",
    "                sentence[1][j+w] = 30\n",
    "            end\n",
    "        end\n",
    "        mean_error_sq = mean_error_sq + (spaces - y_valid[i])^2\n",
    "    end\n",
    "    return mean_error_sq/x_valid:size(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_size = X_train:size(2)\n",
    "ngram_space, ngram_total = count_train(X_train, Y_train, win_size)\n",
    "X_valid_nospaces = pre_pad(X_valid_nospaces, win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COUNT BASED MODEL\t\n",
       "================================\t\n",
       "Window size: 2\t\n",
       "Laplace smoothing parameter: 1\t\n",
       "Perplexity: 1.2736517323433\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additive = 1\n",
    "perplexity = count_perp(ngram_space, ngram_total, X_valid, Y_valid, win_size, additive, nfeatures)\n",
    "\n",
    "print(\"COUNT BASED MODEL\")\n",
    "print(\"================================\")\n",
    "print(\"Window size: \" .. tostring(win_size))\n",
    "print(\"Laplace smoothing parameter: \" .. tostring(additive))\n",
    "print(\"Perplexity: \" .. tostring(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.37\n",
    "additive = 1\n",
    "print(count_greedy_predict(ngram_space, ngram_total, X_valid_nospaces, Y_valid_spaces, win_size, nfeatures, additive, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function count_dp_predict(space_table, total_table, x_valid, y_valid, w, n, a, p)\n",
    "    local mean_error_sq = 0\n",
    "    for i=1,1 do\n",
    "        --This is our tabulated (or memoized I keep forgetting which is which) array to keep track\n",
    "        --of the space probabilities\n",
    "        local array = torch.Tensor(x_valid:size(2)+1):fill(0)\n",
    "        \n",
    "        --We want to set the very first index as 1 because we are multiplying probabilities and the rest\n",
    "        --are 0 because they will be filled in as we go along\n",
    "        array[1] = 2\n",
    "        \n",
    "        \n",
    "        local spaces = 0\n",
    "        local prev_space = 1\n",
    "        local sentence = x_valid:sub(i,i,1,x_valid:size(2)):clone()\n",
    "        for j=1,x_valid:size(2)-w+1 do\n",
    "            if sentence[1][j+w] == 30 then\n",
    "                break\n",
    "            end\n",
    "            local context = sentence:sub(1,1,j,j+w-1)\n",
    "            local key = createHash2(context)\n",
    "            local p_space = 0\n",
    "            local p_nospace = 0\n",
    "            if total_table[key] then\n",
    "                p_space = laplace_smooth(space_table[key], total_table[key], a, n)\n",
    "                p_nospace = laplace_smooth(total_table[key] - space_table[key], total_table[key], a, n)\n",
    "                p_space = normalize(p_space, p_nospace)\n",
    "            end\n",
    "            if p_space > .2 then\n",
    "                \n",
    "                print(p_space, array[j+w], prev_space, p_space, array[prev_space])\n",
    "                if math.exp(p_space)+array[prev_space] > array[j+w] then\n",
    "                    array[j+w] = math.exp(p_space)+array[prev_space]\n",
    "                end\n",
    "                prev_space = j+w\n",
    "                spaces = spaces + 1\n",
    "                --Loop through backwards from the second to last entry up to position of next space\n",
    "                for k=1,sentence:size(2)-j-w do\n",
    "                    local idx = sentence:size(2) - k\n",
    "                    sentence[1][idx+1] = sentence[1][idx]\n",
    "                end\n",
    "                sentence[1][j+w] = 30\n",
    "            end\n",
    "        end\n",
    "        mean_error_sq = mean_error_sq + (spaces - y_valid[i])^2\n",
    "        print(array)\n",
    "    end\n",
    "    return mean_error_sq/1\n",
    "end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42668325299331\t0\t1\t0.42668325299331\t2\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.24459041731066\t0\t5\t0.24459041731066\t3.5321672754575\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.65761689291101\t0\t10\t0.65761689291101\t4.8092654042399\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.47960848287113\t0\t13\t0.47960848287113\t6.7394524109882\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.21061475983756\t0\t17\t0.21061475983756\t8.3548942162154\t\n",
       "0.65659706796979\t0\t21\t0.65659706796979\t9.5893309250654\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.32189270243252\t0\t25\t0.32189270243252\t11.517550482361\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.72935829455687\t0\t30\t0.72935829455687\t12.897287207981\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25089605734767\t0\t34\t0.25089605734767\t14.971036652287\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.31383855024712\t0\t37\t0.31383855024712\t16.256213145022\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.42668325299331\t0\t42\t0.42668325299331\t"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17.6248818925\t\n",
       "0.44319460067492\t0\t47\t0.44319460067492\t19.157049167957\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.24210526315789\t0\t50\t0.24210526315789\t20.714724597497\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.2483099323973\t0\t52\t0.2483099323973\t21.988652880969\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.31383855024712\t0\t56\t0.31383855024712\t23.270510040666\t\n",
       "0.32692307692308\t0\t60\t0.32692307692308\t24.639178788143\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.22956645344705\t0\t66\t0.22956645344705\t26.02587359239\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.23033707865169\t0\t69\t0.23033707865169\t27.283928058891\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.72935829455687\t0\t72\t0.72935829455687\t28.542952387525\t\n",
       "0.78571428571429\t0\t75\t0.78571428571429\t30.616701831832\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25\t0\t78\t0.25\t32.810675336943\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.32692307692308\t0\t79\t0.32692307692308\t34.094700753631\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  2.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  3.5322\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  4.8093\n",
       "  0.0000\n",
       "  0.0000\n",
       "  6.7395\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  8.3549\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  9.5893\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 11.5176\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 12.8973\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 14.9710\n",
       "  0.0000\n",
       "  0.0000\n",
       " 16.2562\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 17.6249\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 19.1570\n",
       "  0.0000\n",
       "  0.0000\n",
       " 20.7147\n",
       "  0.0000\n",
       " 21.9887\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 23.2705\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 24.6392\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       " 26.0259\n",
       "  0.0000\n",
       "  0.0000\n",
       " 27.2839\n",
       "  0.0000\n",
       "  0.0000\n",
       " 28.5430\n",
       "  0.0000\n",
       "  0.0000\n",
       " 30.6167\n",
       "  0.0000\n",
       "  0.0000\n",
       " 32.8107\n",
       " 34.0947\n",
       "  0.0000\n",
       "  0.0000\n",
       " 35.4814\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "[torch.DoubleTensor of size 418]\n",
       "\n",
       "81\t\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.37\n",
    "additive = 1\n",
    "print(count_dp_predict(ngram_space, ngram_total, X_valid_nospaces, Y_valid_spaces, win_size, nfeatures, additive, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
